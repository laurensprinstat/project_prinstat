---
title: "Appendix"
author: Robin Boudry, Maarten Rahier, Tom Schipper, Laurens Van Paemel
output:
  word_document:
    highlight: "tango"
    toc: no
---

# Preparation

```{r}
# Used libraries
library(plyr)
```

Importing and cleaning the data

```{r}
# Read in data
data <- read.table("bptrial.txt", header=TRUE, sep=",", dec=".")
original <- data
# Set column names
colnames(data) <- c("treatment", "sex", "weight", "age",
                    "comp", "dbpdif", "dbp3", "dbp2", "dbp1")
# Add column dbp_end for the 5th measurement at the end of the experiment
data$dbp_end <- data$dbp3 + data$dbpdif
# Mean dbp during the run-in period
data$dbp_mean <- round((data$dbp1 + data$dbp2 + data$dbp3) / 3, 1)
# Raw data subset
data_raw <- data
# Treatment group as factor
data$treatment <- as.factor(mapvalues(
  data$treatment,
  from=c(1, 2, 3),
  to=c("Treatment 1", "Placebo", "Treatment 3")
))
# Gender as factor
data$sex <- as.factor(mapvalues(data$sex, c(0, 1), c("Female", "Male")))
# 80% adherence rule
data <- data[data$comp >= 0.8,]
##TS: don't we remove the patient with a blood pressure of 66?
data=data[data$dbp3 != 66,]
# Subset of treatment 1 and the placebo
data12 <- data[data$treatment != "Treatment 3",]
levels(data12$treatment) <- c("Treatment 1", "Placebo", NA)
# Seperate subsets of treatment 1 and placebo
data1 <- data[data$treatment == "Treatment 1",]
data2 <- data[data$treatment == "Placebo",]
```

#######################
# 1. Power comparison #
#######################

In the code, trailing 1's are a reference to "Treatment 1" and 2's to "Placebo".

```{r}
# Amount of observations
n <- nrow(data12)
n1 <- summary(data12$treatment)[["Treatment 1"]]
n2 <- summary(data12$treatment)[["Placebo"]]

##############################
# Simulating the dbpdif data #
##############################

# Checking normality
qqnorm(data1$dbpdif); qqline(data1$dbpdif)
## Skewed to the left?
qqnorm(data2$dbpdif); qqline(data2$dbpdif)
## Short tail on the right?

# Testing the homogeneity of variances
var.test(data1$dbpdif, data2$dbpdif)
## Although just not significant, the ratio of the variances
## seems to deviate from 1. It would be safer to assume that
## the variances are not equal.

# Observed means
mu_dif1 <- mean(data1$dbpdif)
mu_dif2 <- mean(data2$dbpdif)
# Observed SD
sd_dif1 <- sd(data1$dbpdif)
sd_dif2 <- sd(data2$dbpdif)
# Resample dbpdif
rdif1 <- rnorm(n1, mu_dif1, sd_dif1)
rdif2 <- rnorm(n2, mu_dif2, sd_dif2)
# Checking for normality
qqnorm(rdif1)
qqline(rdif1)
qqnorm(rdif2)
qqline(rdif2)
## QQplots from data generated randomly from normal distributions
## show similar deviations as the QQplots of the data

###############################
# Simulating the dbp_end data #
###############################

# Checking normality
qqnorm(data1$dbp_end); qqline(data1$dbp_end)
#1 outlier to the left?
qqnorm(data2$dbp_end); qqline(data2$dbp_end)
#Short tail on the left?
#The dbp_end seems to show fewer deviations from normality than dbpdif

# Observed means
mu_end1 <- mean(data1$dbp_end)
mu_end2 <- mean(data2$dbp_end)
# SD
sd_end1 <- sd(data1$dbp_end)
sd_end2 <- sd(data2$dbp_end)

# Resample dbp_end
rend1 <- rnorm(n1, mu_end1, sd_end1)
rend2 <- rnorm(n2, mu_end2, sd_end2)
# Check normality
qqnorm(rend1); qqline(rend1)
qqnorm(rend2); qqline(rend2)
## QQplots from data generated randomly from normal distributions
## show similar deviations as the QQplots of the data

###############
# Permutation #
###############

## 10000 simulations for each of 41 differences in means, for both
## dbpdif and dbp_end. These are plotted together for comparison

# Amount of simulations
n.sim <- 10000
# Collect results
pwr_end <- vector(,41)
pwr_dif <- vector(, 41)
# Set seed for reproducibility
set.seed(2018)
# Test 41 differences in means
for(y in 0:40){
  x <- 0.025 * y
  # Array of p-values
  pdif <- vector(, n.sim)
  pend <- vector(, n.sim)
  # Permutate
  for(i in 1:n.sim) {
    # dbp_dif
    mu_dif1.2 <- mu_dif1 + (abs(mu_dif1) - abs(mu_dif2)) * x
    rdif1 <- rnorm(n1, mu_dif1.2, sd_dif1)
    rdif2 <- rnorm(n2, mu_dif2, sd_dif2)
    tdif <- t.test(rdif1, rdif2, "less", mu=0, paired=F, var.equal=F)
    pdif[i] <- tdif$p.value
    # dbp_end
    mu_end1.2 <- mu_end1 + (mu_end2 - mu_end1) * x
    rend1 <- rnorm(n1, mu_end1.2, sd_end1)
    rend2 <- rnorm(n2, mu_end2, sd_end2)
    tend <- t.test(rend1, rend2, "less", mu=0, paired=F, var.equal=F)
    pend[i] <- tend$p.value
  }
  pwr_dif[y+1] <- mean(pdif < 0.05)
  pwr_end[y+1] <-mean(pend < 0.05)
}

# Plotting both power simulations in the same graph
plot(pwr_dif, type="n", xaxt="n", main="Comparative power", yaxt="n",
     ylab="Power (%)", xlab="Difference in means (% observed difference)")
lines(rev(pwr_dif), lty=1, col="blue")
lines(rev(pwr_end), pch=1, lty=2, col="red")
axis(side=1, at=seq(from=0, to=40, by=4), labels=seq(from=0, to=100, by=10))
axis(side=2, at=seq(from=0, to=1, by=0.2), labels=seq(from=0, to=100, by=20))
legend(33.77, 1.036, legend=c("dbp_dif", "dbp_end"),
       col=c("blue", "red"), lty=1:2, cex=1.2)
```

##################
#Question 2 (TS) #
##################
```{r}
##Analytical approach to the power of the t-test
alpha <- 0.05
n <- n1 + n2
#Alpha is not divided by 2, as we test one-sided
t.alpha <- qt(1-alpha, df = n - 2)

#With dbpdif, where the difference between the groups is 42.5% of the observed difference
delta_dif <- -(mu_dif1 - mu_dif2) * 0.425
lambda_dif <- delta_dif /  sqrt(var(data$dbpdif[data$treatment == "Treatment 1"])/n1 + 
                                      var(data$dbpdif[data$treatment == "Placebo"])/n2)
1 - pt(t.alpha, df = n - 2, ncp = lambda_dif)

#With dbp_end, where the difference between the groups is 42,5% of the observed difference
delta_end <- -(mu_end1 - mu_end2) * 0.425
lambda_end <- delta_end /  sqrt(var(data$dbp_end[data$treatment == "Treatment 1"])/n1 + 
                                      var(data$dbp_end[data$treatment == "Placebo"])/n2)
1 - pt(t.alpha, df = n - 2, ncp = lambda_end)

#Non-parametric alternative: The l (lambda) is much too large, this doesn't work
l_dif.2 <- sqrt((12 * n1 * n2) / (n + 1)) * delta_dif.2
1 - pt(t.alpha, df = n - 2, ncp = l_dif.2)

######
#RB power function to compare hard differences
######
powertest <- function(alpha = 0.05, variable1, variable2, delta_dif){
n1 <-length(variable1)
n2 <- length(variable2)
n <- n1+n2
#Alpha is not divided by 2, as we test one-sided
t.alpha <- qt(1-alpha, df = n - 2)
#With dbpdif, where the difference between the groups is 42.5% of the observed difference
lambda_dif <- delta_dif /  sqrt(var(variable1)/n1 + var(variable2)/n2)
print(1 - pt(t.alpha, df = n - 2, ncp = lambda_dif))
}

powertest(alpha = 0.05, data$dbpdif[data$treatment == "Treatment 1"], data$dbpdif[data$treatment == "Placebo"], delta_dif = 3)
powertest(alpha = 0.05, data$dbp_end[data$treatment == "Treatment 1"], data$dbp_end[data$treatment == "Placebo"], delta_dif = 3)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   5)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 5)
```

###################
# Question 3 (LS) #
###################

```{r}
n_placebo <- summary(data12$treatment)[["Placebo"]]
n_treatment <- summary(data12$treatment)[["Treatment 1"]]
mean_placebo <- mean(data12[data12$treatment == "Placebo",]$dbp_end)
mean_treatment <- mean(data12[data12$treatment == "Treatment 1",]$dbp_end)
#mean_difference <- mean_treatment - mean_placebo
sd_placebo <- sd(data12[data12$treatment == "Placebo",]$dbp_end)
sd_treatment <- sd(data12[data12$treatment == "Treatment 1",]$dbp_end)

n <- 1000000
power <- vector(, n)
type1 <- vector(, n)
set.seed(1991)
for(i in 1:n){
  r_placebo <- rnorm(n=n_placebo, mean=mean_placebo, sd=sd_placebo)
  r_treat <- rnorm(n=n_treatment, mean=mean_treatment, sd=sd_treatment)
  r_placebo_0 <- rnorm(n=n_placebo, mean=0, sd=sd_placebo)
  r_treat_0 <- rnorm(n=n_treatment, mean=0, sd=sd_treatment)
  # One sided pooled t-test
  test <- t.test(r_treat, r_placebo, alternative="less", var.equal=FALSE)
  test_0 <- t.test(r_treat_0, r_placebo_0, alternative="greater", var.equal=FALSE)
  power[i] <- test$p.value
  type1[i] <- test_0$p.value
}
# Power
mean(power < 0.05)
# Type 1
mean(type1 < 0.05)
```

[1] 0.999900
Approx power approaching 100% or >99.9%

[1] 0.050319
Type 1 error rate of approx 5%?

#(TS) Dit was mijn idee van vraag 3
```{R}
#dbp_end and dbp_dif are simulated simultaneously, but dependent on each other, under H0 and both are tested for a significant effect.
#For each simulation, the lowest p-value of the two
mu_end <- mean(data12$dbp_end)
mu_base <- mean(data12$dbp3)
sd_base1 <- sd(data12[data12$treatment == "Treatment 1",]$dbp3)
sd_base2 <- sd(data12[data12$treatment == "Placebo",]$dbp3)

p_min <- vector(, 10000)
p_end <- vector(, 10000)
p_dif <- vector(, 10000)
set.seed(2242)
for(i in 1 : 10000) {
  sim_end1 <- rnorm(n1, mu_end, sd_end1)
  sim_end2 <- rnorm(n2, mu_end, sd_end2) #the means in both groups are equal to simulate H0
  sim_base1 <- rnorm(n1, mu_base, sd_base1)
  sim_base2 <- rnorm(n2, mu_base, sd_base2)
  sim_dif1 <- sim_end1 - sim_base1 #sim_dif is calculated as a function of sim_end to simulate dependence
  sim_dif2 <- sim_end2 - sim_base2
  t_test_end <- t.test(sim_end1, sim_end2, alternative = "less", mu = 0, paired = F, var.equal = F)
  t_test_dif <- t.test(sim_dif1, sim_dif2, alternative = "less", mu = 0, paired = F, var.equal = F)
  p_end[i] <- t_test_end$p.value
  p_dif[i] <- t_test_dif$p.value
  p_min[i] <- min(c(t_test_end$p.value, t_test_dif$p.value))
}
#Type I error rate is well-controlled for the individual variables
mean(p_end < 0.05)
mean(p_dif < 0.05)
#However, the Type I error rate is too high when the lowest p-value of the two is used.
mean(p_min < 0.05)
```
# Something

```{r}
n_p <- summary(data12$treatment)[["Placebo"]]
n_t <- summary(data12$treatment)[["Treatment 1"]]

# observed means
mean_p_end <- mean(data12[data12$treatment == "Placebo",]$dbp_end)
mean_t_end <- mean(data12[data12$treatment == "Treatment 1",]$dbp_end)
dif_end <- mean_p_end - mean_t_end
mean_p_dif <- mean(data12[data12$treatment == "Placebo",]$dbpdif)
mean_t_dif <- mean(data12[data12$treatment == "Treatment 1",]$dbpdif)
dif_dif <- mean_p_dif - mean_t_dif

# T-test settings
var_eq <- FALSE

# Permutation settings
n_sim <- 10000
tt_end <- rep(NA, n_sim)
tt_dif <- rep(NA, n_sim)
pdata <- data12
set.seed(1991)

# Permutation
for(i in 1:n_sim){
  pdata$treatment <- sample(pdata$treatment)
  tt_end[i] <- t.test(dbp_end ~ treatment, data=pdata, alternative="less", var.equal=var_eq)$statistic
  tt_dif[i] <- t.test(dbpdif ~ treatment, data=pdata, alternative="less", var.equal=var_eq)$statistic
}

# Whilutation settings
p_end <- 1
p_dif <- 1
adj = 0

data_w <- data12
# Whilutation until p-value of 0.05
while (min(c(p_end, p_dif)) > 0.05) {
  # Difference in means
  adj <- round(adj + 0.01, 2)
  # Equalize means and add difference
  adj_t_end <- data12[data12$treatment == "Treatment 1",]$dbp_end + dif_end - adj
  placebo_end <- data12[data12$treatment == "Placebo",]$dbp_end
  adj_t_dif <- data12[data12$treatment == "Treatment 1",]$dbpdif + dif_dif - adj
  placebo_dif <- data12[data12$treatment == "Placebo",]$dbpdif
  # Adjusted means t-test
  tt_obs_end <- t.test(adj_t_end, placebo_end, alternative="less", var.equal=var_eq)$statistic
  tt_obs_dif <- t.test(adj_t_dif, placebo_dif, alternative="less", var.equal=var_eq)$statistic
  # P-value
  p_end <- mean(tt_end < tt_obs_end)
  p_dif <- mean(tt_dif < tt_obs_dif)
}
# Result
paste("Smallest detected difference:", round(adj, 2))
paste("P-value final measure:", p_end)
paste("P-value difference dbp5-3:", p_dif)
```

[1] "Smallest detected difference: 3.14"
[1] "P-value final measure: 0.0591"
[1] "P-value difference dbp5-3: 0.0481"

```{r}
# Whilutation settings
j <- 0
adj <- 0
adj_max <- 12
adj_step <- 0.01
continue <- TRUE

# Collect all data for graph (until p-value approaches 0)
while (adj <= adj_max && continue) {
  adj <- round(adj_step * j, 2)
  j <- j + 1
  # Equalize means and set difference
  adj_t_end <- data12[data12$treatment == "Treatment 1",]$dbp_end + dif_end - adj
  adj_t_dif <- data12[data12$treatment == "Treatment 1",]$dbpdif + dif_dif - adj
  placebo_end <- data12[data12$treatment == "Placebo",]$dbp_end
  placebo_dif <- data12[data12$treatment == "Placebo",]$dbpdif
  # Adjusted difference observed value
  tt_obs_end <- t.test(adj_t_end, placebo_end, alternative="less", var.equal=var_eq)$statistic
  tt_obs_dif <- t.test(adj_t_dif, placebo_dif, alternative="less", var.equal=var_eq)$statistic
  #test_0 <- t.test(r_treat_0, r_placebo_0, alternative="greater", var.equal=FALSE)
  # P-value
  p_end[j] <- mean(tt_end < tt_obs_end)
  p_dif[j] <- mean(tt_dif < tt_obs_dif)
  # Stop if both approach 0
  continue <- p_end[j] != 0 || p_dif[j] != 0
  adj_stop <- adj
}
```

```{r}
# Plot the result
ymax <- max(p_end, p_dif)
plot(p_dif, col="blue", type="l", xaxt="n", xlab="Difference in means", ylab="P-value", main="P-value by difference in means", ylim=c(0, ymax))
lines(p_end, col="green")
abline(0.05, 0, col="red")
axis(side = 1, at=seq(0, 700, 100), labels=seq(0, 7, 1))
```

```{r}
sd_p_dif <- sd(data12[data12$treatment == "Placebo",]$dbpdif)
sd_t_dif <- sd(data12[data12$treatment == "Treatment 1",]$dbpdif)

n_sim <- 1000000
r_power <- rep(NA, n_sim)
r_type1 <- rep(NA, n_sim)
set.seed(1991)

for(i in 1:n_sim){
  r_placebo <- rnorm(n=n_p, mean=mean_p_dif, sd=sd_p_dif)
  r_treat <- rnorm(n=n_t, mean=mean_t_dif + dif_dif - adj, sd=sd_t_dif)
  r_placebo_0 <- rnorm(n=n_p, mean=0, sd=sd_p_dif)
  r_treat_0 <- rnorm(n=n_t, mean=0, sd=sd_t_dif)
  # One sided pooled t-test
  test <- t.test(r_treat, r_placebo, alternative="less", var.equal=var_eq)
  test_0 <- t.test(r_treat_0, r_placebo_0, alternative="greater", var.equal=var_eq)
  r_power[i] <- test$p.value
  r_type1[i] <- test_0$p.value
}
# Power
mean(r_power < 0.05)
# Type 1
mean(r_type1 < 0.05)
```

###################################################
# Q 4                                            #
###################################################

```{R}
#dbp_end and dbp_dif are simulated simultaneously, but dependent on each other, under H0 and both are tested for a significant effect.
#For each simulation, the lowest p-value of the two
mu_end <- mean(data12$dbp_end)
mu_base <- mean(data12$dbp3)
sd_base1 <- sd(data12[data12$treatment == "Treatment 1",]$dbp3)
sd_base2 <- sd(data12[data12$treatment == "Placebo",]$dbp3)

p_min <- vector(, 10000)
p_end <- vector(, 10000)
p_dif <- vector(, 10000)
p_and <- vector(, 10000)
set.seed(2242)
for(i in 1 : 10000) {
  sim_end1 <- rnorm(n1, mu_end, sd_end1)
  sim_end2 <- rnorm(n2, mu_end, sd_end2) #the means in both groups are equal to simulate H0
  sim_base1 <- rnorm(n1, mu_base, sd_base1)
  sim_base2 <- rnorm(n2, mu_base, sd_base2)
  sim_dif1 <- sim_end1 - sim_base1 #sim_dif is calculated as a function of sim_end to simulate dependence
  sim_dif2 <- sim_end2 - sim_base2
  t_test_end <- t.test(sim_end1, sim_end2, alternative = "less", mu = 0, paired = F, var.equal = F)
  t_test_dif <- t.test(sim_dif1, sim_dif2, alternative = "less", mu = 0, paired = F, var.equal = F)
  p_end[i] <- t_test_end$p.value
  p_dif[i] <- t_test_dif$p.value
  #p_end[i] <- p.adjust(t_test_end$p.value, method="bonferroni", n=2)
  #p_dif[i] <- p.adjust(t_test_dif$p.value, method="bonferroni", n=2)
  p_min[i] <- min(c(t_test_end$p.value, t_test_dif$p.value))
  p_and[i] <- max(c(t_test_end$p.value, t_test_dif$p.value))
}
#Type I error rate is well-controlled for the individual variables
mean(p_end < 0.05)
mean(p_dif < 0.05)
#However, the Type I error rate is too high when the lowest p-value of the two is used.
mean((p_min * 2) < 0.05)
```
