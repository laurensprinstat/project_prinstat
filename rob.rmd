---
title: "R Notebook"
output: html_notebook
---

Load packages
```{r}
library(plyr)
library(coin)
```

I. Read in the data & clean
```{r}
# Read in data
data <- read.table("bptrial.txt", header=TRUE, sep=",", dec=".")
original <- data
# Set column names
colnames(data) <- c("treatment", "sex", "weight", "age",
                    "comp", "dbpdif", "dbp3", "dbp2", "dbp1")
# Add column dbp_end for the 5th measurement at the end of the experiment
data$dbp_end <- data$dbp3 + data$dbpdif
# Mean dbp during the run-in period
data$dbp_mean <- round((data$dbp1 + data$dbp2 + data$dbp3) / 3, 1)
# Raw data subset
data_raw <- data
# Treatment group as factor
data$treatment <- as.factor(mapvalues(
  data$treatment,
  from=c(1, 2, 3),
  to=c("Treatment 1", "Placebo", "Treatment 3")
))
# Gender as factor
data$sex <- as.factor(mapvalues(data$sex, from=c(0, 1), to=c("Female", "Male")))
# 80% adherence rule
data <- data[data$comp >= 0.8,]
##TS: don't we remove the patient with a blood pressure of 66?
data = data[data$dbp3 != 66,]
# Subset of treatment 1 and the placebo
data12 <- data[data$treatment != "Treatment 3",]
levels(data12$treatment) <- c("Treatment 1", "Placebo", NA)
```

1. A simulation study should be conducted to assess which variable is best used for analysis:
Two variable tests should be compared here to quantify the difference between treatment 1 & 2,:
a. 'use the blood pressure measured at the end' (dbp5)
b. 'the change from baseline' (dbpend)
Which one is preferable should be concluded from the Power or Precision.
We will compute the power based on a 0.05 test statistic for both and compare.
In accordance with our analysis in the first part, we will rely on the Central Limit Theorum to justify the use of a t-test.


1. Compute the power for treatment 1 vs treatment 2 looking at (1)dbp5 'aka dbp_end' & (2) dbpdif,
based on simulated data 
```{r}
#both groups
data12 <- data[data$treatment != "Treatment 3",]

#individual groups
treat1 <- data[data$treatment == "Treatment 1",]
treat2 <- data[data$treatment == "Placebo",]

#looking at qqplots
#make a function to generate named plots of variable of choice
qqplotter <- function(variable){
  qqnorm(variable, 
         main= c(paste('QQPlot of:',deparse(substitute(variable))))
         )
  ; qqline(variable)
}

#looking at dbp_end
qqplotter(treat1$dbp_end)
qqplotter(treat2$dbp_end)

#looking at dbpdif
qqplotter(treat1$dbpdif)
qqplotter(treat2$dbpdif)

#We can conclude the tails are problematic to assume a normal distribution.
#However, for each variable in each group we find support for the central Limit Theorum, and thus can use a t-test
length(treat1$dbp_end)>30
length(treat2$dbp_end)>30
length(treat1$dbpdif)>30
length(treat2$dbpdif)>30

#Looking at the variances
#equal variances have a a ratio of 1
#It is better to use a Welch t test
var.test(treat1$dbpdif, treat2$dbpdif)   #marginally significant evidence against H0 (0.056)
var.test(treat1$dbp_end, treat2$dbp_end) #doubtfull evidence, but a ratio of 0.61 enough to assume unequal

#Get means & sd to simulate data
meansd <- function(variable){
  return(c(
    mean(variable),
    sd(variable),
    length(variable)
    ))
}

meansd(treat1$dbp_end)
meansd(treat2$dbp_end)
meansd(treat1$dbpdif)
meansd(treat2$dbpdif)   #note here that the largest difference in observed means is no more than |13|

#simulate 1ce to check if distributions are comparable
#the generations is random, when executing the simulation multiple times,
#we see that some mimic the data, some seem quite different (as to be expected)
simqq <- function(variable){
  simulated_distr <- rnorm(n = length(variable),
        mean = mean(variable),
        sd = sd(variable))
  par(mfrow=c(1,2))
  qqplotter(variable)
  qqplotter(simulated_distr)
  par(mfrow=c(1,1))
}
simqq(treat1$dbp_end)
simqq(treat2$dbp_end)
simqq(treat1$dbpdif)
simqq(treat2$dbpdif)

#start actual power simulations

#(dbp5) 1 sided testing, treatment 1 bp mean is expected to be below (smaller) treat 2 
#'H0' ->  'mean(treat1$dbp_end)' = 'mean(treat2$dbp_end)' ; 'HA' ->  'mean(treat1$dbp_end)' < 'mean(treat2$dbp_end)'
#(dbp_dif) 1 sided testing, the diff mean in treat 1 is expected to be more negative (smaller) than in treat 2
#'H0' ->  'mean(treat1$dbp_dif)' = 'mean(treat2$dbp_dif)' ; 'HA' ->  'mean(treat1$dbp_dif)' < 'mean(treat2$dbp_dif)'

#There are 2 ways we will look at to influence power
#1 look at the difference value
#2 Look at the sample size

#1 difference value
#create a power loop in a functio,, to be reused for dbpdif



power_f_delta<-function(N.sim, variable1, variable2){
pdif <-vector()
power <- vector()
meandif <- vector()
mean1 <- vector()
mean2 <- vector()
for(y in 1:60){
  x <- c(y-60)*0.2 # from looking at the means, we know they differ no more than |13|. So we will start by equalising the means and building up the difference as the simulations go along 
  for(i in 1:N.sim){
    X1 <- rnorm(n = length(variable1),
                mean = mean(variable1),
                sd = sd(variable1))
    X2 <- rnorm(n = length(variable2),
                mean = mean(variable2)+x,  #here we start by comparing a 0 difference µ=µ to a difference of µ+50
                sd = sd(variable2))
    test <- t.test(X1, X2, alternative = 'less', var.equal = FALSE)
    pdif[i] <- test$p.value
    mean1[i] <- mean(X1)
    mean2[i] <- mean(X2)
  }
  power[y]<-mean(pdif < 0.05)
  meandif[y]<-mean(mean1)-mean(mean2)
 }
return(cbind(meandif,power))
}

dbpend_power_sim <- power_f_delta(1000, treat1$dbp_end, treat2$dbp_end)

dbpdif_power_sim <-power_f_delta(1000, treat1$dbpdif, treat2$dbpdif)


plot(dbpend_power_sim, type="l",col='red',
     xlab = "Difference in means (observed mean difference of simulations)",
     main = "Comparative power, fixed Delta",
      ylab = "Power")
lines(dbpdif_power_sim, col='blue')

```
Conclusion 1: From the simulation plot we can clearly see that dbpdif has more power

2. Use analytical calculations to develop insights into when and why it is preferable 
```{r}
##Analytical approach to the power of the t-test

powertest <- function(alpha = 0.05, variable1, variable2, delta_dif){
n1 <-length(variable1)
n2 <- length(variable2)
n <- n1+n2
#Alpha is not divided by 2, as we test one-sided
t.alpha <- qt(1-alpha, df = n - 2)
#With dbpdif, where the difference between the groups is 42.5% of the observed difference
lambda_dif <- delta_dif /  sqrt(var(variable1)/n1 + var(variable2)/n2)
print(1 - pt(t.alpha, df = n - 2, ncp = lambda_dif))
}

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   6)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 6)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   5)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 5)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif, 4.141273)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 4.141273)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   3)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 3)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   2)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 2)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   1)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, 1)

powertest(alpha = 0.05, treat1$dbpdif, treat2$dbpdif,   -1)
powertest(alpha = 0.05, treat1$dbp_end, treat2$dbp_end, -1)

#Check the effect of variance
print(1 - pt(t.alpha, df = n - 2, ncp = 4))
print(1 - pt(t.alpha, df = n - 2, ncp = 2))
#NCP -> lambda_dif <- delta_dif /  sqrt(var(variable1)/n1 + var(variable2)/n2)

#'The smaller the variance, the higher the power.
#'Lets optimalise the variance

#create sigmas
sigmatest <- vector()

sigma <- 12
for(r in 1:500){
  sigma <- sigma-(0.001*r)
  sigmatest[r] <- sigma
}
sigmatest <- sigmatest[sigmatest>0]

sigma.test <- vector()
pval.sim <- vector()

#diff 3
 for(i in 1:length(sigmatest)){
   X1<-rnorm(39, 0, sigmatest[i])
   X2<-rnorm(44, 3, sigmatest[i])
   sigma.test[i] <- sigmatest[i]
   pval.sim[i] <- t.test(X1, X2, alternative = 'less', var.equal = TRUE)$p.value
  }
plot(sigmatest, pval.sim)
abline(h = 0.05, col= 'red')

#diff 1
 for(i in 1:length(sigmatest)){
   X1<-rnorm(39, 0, sigmatest[i])
   X2<-rnorm(44, 1, sigmatest[i])
   sigma.test[i] <- sigmatest[i]
   pval.sim[i] <- t.test(X1, X2, alternative = 'less', var.equal = TRUE)$p.value
  }
plot(sigmatest, pval.sim)
abline(h = 0.05, col= 'red')

#This plot shows again that smaller 

```

It becomes clear that for hard differences(2,3,4) the dbpdif has more merit.
When the NCP goes up, so does the power. If we test for the same difference, the differentiating factor in dbp_end and dbp_diff is the variance. We see the dbp_dif having slightly less variance, so the Delta gets divided by a smaller number -> higher ncp -> higher power.
Because the difference in dbp5 and dbpdiff is not a constant, but a random variable(the dbp of subject)




3. What would be the type 1 error rate, using the analytical approach?
We can conclude that the smallest variance would lead to the largest power
Let's look at the true error rate for a number of variances
The true error rate is the proportion of observed p<0.05 for a random distribution
```{r}
#create sigmas
sigmatest <- vector()

sigma <- 12
for(r in 1:5000){
  sigma <- sigma-(0.0001*r)
  sigmatest[r] <- sigma
}
sigmatest <- sigmatest[sigmatest>0]

sigma.test <- vector()
pval.sim <- vector()

sigmat <- vector()
true0 <- vector()

 for(i in 1:length(sigmatest)){
   sigmat <- sigmatest[i]
   for(y in 1:1000){
   X1<-rnorm(39, 0, sigmat)
   X2<-rnorm(44, 0, sigmat)
   pval.sim[y] <- t.test(X1, X2, alternative = 'less', var.equal = TRUE)$p.value
   #mean(pval.sim<0.05))
  }
   true0[i]  <- mean(pval.sim<0.05)
   sigmat[i] <- sigmat
 } 

plot(sigmatest, true0)
cor(sigmatest, true0)

abline(h = 0.05, col= 'red')

```

The simulation above shows that the 0.05 treshold is not guaranteed purely looking at the variance. As to be expected, the p-value acts random

4. How to ensure that the right sign level is attained?
Knowing that the p-value behaves randomly when using random samples, we should keep in mind that a single observation does not give conclusive answers. If the variance is less, we have more chance of a more extreme p-value and a higher power.
Therefore if we have observed a smaller p-value during a single analysis, it is worth looking into the variance and how this smaller p-value holds up during simulation if the variable with larger p-value has a smaller variance.

4
```{r}
print('to do: calculate the p-values for dif and 5 single time, see which variance is smaller, make loop 10000 with same Delta and smallest variance should have best p-value')
```


